[["index.html", "Interprétabilité de boîtes noires Chapitre 1 Introduction Remerciements", " Interprétabilité de boîtes noires Lucie Guillaumin &amp; Mehdi Chebli 2021-01-04 Chapitre 1 Introduction Le machine learning (apprentissage automatique en français) est une technologie dintelligence artificielle, qui à laide dapproches mathématiques et surtout statistiques permet de donner la capacité aux machines dapprendre à partir des données. Cest-à-dire daméliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacunes. L'objectif de ces algorithmes consiste à estimer un modèle à partir des observations puis prédire une variable que l'on souhaite expliquer. Certains de ces modèles de machine learning sont appelés boîtes noires. En effet, après application du modèle, on obtient des résultats mais nous ne savons pas comment ni pourquoi la machine est parvenue à ce résultat. Dans ce mémoire, nous cherchons donc à expliquer et à comprendre comment et pourquoi un modèle de boite noire donne tel ou tel résultat. Les différents chapitres que contiennent ce livre expliqueront donc comment résoudre ce problème appelé interprétabilité. On peut trouver différentes définitions de linterprétabilité sur le net, celles que nous retiendrons sont les suivantes : la capacité dans laquelle un être humain peut comprendre la cause dune décision le degré auquel un être humain peut prédire de manière cohérente le résultat dun modèle Linterprétabilité est très importante dans le monde des modèles de machine learning. En réalité, dans de nombreux domaines il faut expliquer/justifier une prise de décision provoquée par le modèle prédictif. Notons quelle nest pas requise si le modèle étudié na pas dimpact significatif : si le modèle possède un impact social ou financier, linterprétabilité devient alors pertinente. De plus, lorsque le modèle est bien étudié il nest pas nécessaire de faire appel à elle. Certains algorithmes disposent de méthodes permettant de déterminer limportance des variables, ils nindiquent cependant pas si une variable affecte positivement ou négativement le modèle. Effectivement, dans la plupart des cas, on ne se soucie généralement pas de savoir pourquoi telle ou telle décision a été prise : on ne cherche quà savoir si la performance prédictive sur un jeu de données de test est correct. Mais il faut néanmoins faire attention : une prédiction correcte ne résout que partiellement notre problème initial, elle ne nous donne aucune information ni explication. Le besoin dinterprétabilité signifie donc quil ne suffit pas dobtenir la prédiction (le quoi) mais plutôt que le modèle doit expliquer comment il en est arrivé à la prédiction (le pourquoi). Mais encore une fois attention, les explications du modèle ne doivent pas expliquer entièrement le déroulement du modèle, ils doivent plutôt aborder une ou deux cause(s) principale(s). Pour cela, nous avons choisi trois méthodes : les graphiques de dépendance partielle, la permutation de variable afin de remarquer les variables les plus importantes dans les modèles prédictif, et la méthode LIME. Nous nous appuierons donc sur des exemples appliqués à des ensembles de données ainsi que sur des modèles de boites noires présentés dans les références du livre. Nous nous baserons sur le livre Interpretable Machine Learning de Christophe Molnar. Remerciements Nous tenons à remercier Mr. Perduca qui nous a permis de bénéficier de son encadrement. Les conseils quil nous a prodigué, la patience, la confiance quil nous a témoignés ont été déterminants dans la réalisation de notre travail. Nos remerciements sétendent également à tous nos enseignants durant les années des études. Enfin, nous tenons à remercier tous ceux qui, de près ou de loin, ont contribué à la réalisation de ce travail. "],["graphique-de-dépendance-partielle-pdp.html", "Chapitre 2 Graphique de dépendance partielle (PDP) 2.1 Définition 2.2 Avantages 2.3 Inconvénients 2.4 En R 2.5 Exemples 2.6 Espérance conditionnelle individuelle (ICE)", " Chapitre 2 Graphique de dépendance partielle (PDP) 2.1 Définition Le graphique de dépendance partielle permet de comprendre les variables importantes dans un modèle donné. Effectivement, le PDP permet d'examiner la relation entre une variable explicative (voire deux) et le modèle à expliquer pour lequel nous calculons des prédictions. Supposons que l'on souhaite comprendre l'importance d'une variable dans un modèle donné. Le PDP construit le modèle en faisant la moyenne des autres variables prédictives, à l'exception d'une variable prédictive choisie : il peut aider à identifier comment une variable affecte notre modèle. En effet, si l'on observe un plateau sur le PDP alors pour les valeurs associées la variable n'est pas utilisée pour prédire : elle n'a pas d'effet dans le modèle. Soit \\(x_S\\) une variable pour laquelle le PDP va être tracé, cette variable est donc fixée. En général, dans l'ensemble de \\(S\\) il n'y a qu'une ou deux caractéristiques. Ces caractéristiques sont celles pour lesquelles on veut connaître l'effet sur la prédiction. Soit \\(x_C\\) toutes les autres variables utilisées dans le modèle de machine learning, ce sont donc ces variables que nous allons faire varier. Soit \\(\\hat{f}\\) la boîte noire que l'on cherche à expliquer. On définit la fonction de dépendance partielle par : \\[ \\hat{f}_{x_S}(x_S) = \\mathbb{E}[\\hat{f}(x_S, x_C)] = \\int \\hat{f}(x_S,x_C) d\\mathbb{P}(x_C) \\] Exemple Soit \\(x = x_S\\) et \\(z = x_C \\in \\{0,1\\}\\). \\[ \\mathbb{E}_z[\\hat{f}(x,z)] = \\hat{f}(x,0)\\mathbb{P}(z=0) + \\hat{f}(x,1)\\mathbb{P}(z=1) \\\\ =\\hat{f}(x) \\text{ : fonction qui dépend de x} \\] En réalité, \\(\\hat{f}(x)\\) définie comment \\(\\hat{f}\\) dépend en &quot;moyenne&quot; de \\(x\\), c'est à dire quand l'on fait varier toutes les autres variables. On définit maintenant la méthode de Monte Carlo. \\[ \\mathbb{E}[g(X)] = \\int g(x)F(x)dx \\text{ où } F(x) \\text{ densité de } X \\] Pour estimer cette espérance, on peut : - tirer \\(x_1,...,x_n\\) selon \\(F\\) - calculer \\(g(x_1),...,g(x_n)\\) - \\(\\hat{\\mathbb{E}}[g(x)] = \\frac{1}{n} \\sum_{i=1}^n g(x_i)\\) A l'aide de la méthode de Monte Carlo, nous pouvons définir la fonction partielle \\(\\hat{f}(x_S)\\) qui est estimée en calculant les moyennes dans les données d'entraînement : \\[ \\hat{f}_{x_S}(x_S) = \\frac{1}{n}\\sum_{i=1}^n \\hat{f}(x_S, x_C^{(i)}) \\] Cette fonction nous indique pour une ou plusieurs valeurs données de \\(S\\) quel est l'effet marginal moyen sur la prédiction. Ici, les \\(x_c^{(i)}\\) sont les valeurs réelles des caractéristiques de l'ensemble de données pour les caractéristiques qui ne nous intéressent pas, et n est le nombre d'occurrences dans l'ensemble de données. Cas particulier Dans le cas particulier où notre modèle n'est pas une boîte noire, la méthode de Monte Carlo illustre bien le fait que nous retombons sur quelque chose que nous connaissons déjà et donc il n'est pas nécessaire d'utiliser un PDP. Nous allons le confirmer avec le choix d'un modèle linéaire comme modèle de machine learning. Soient \\(\\hat{f}(x,z) = a+bx+cz\\) et \\(\\psi\\) la densité de \\(z\\). \\[ \\int\\hat{f}(x,z)\\psi(z)dz = \\int (a+bx+cz)\\psi(z)dz \\\\ = a\\int\\psi(z)dz + bx\\int\\psi(z)dz + c\\int z\\psi(z)dz \\\\ = a + bx + c \\mathbb{E}[z] \\] On utilise donc la méthode de Monte Carlo pour estimer \\(\\mathbb{E}[z]\\), et c'est quelque chose que l'on sait faire aisément. 2.2 Avantages Les graphiques de dépendance partielle sont faciles à implémenter et à interpréter. Ils fonctionnent pour les variables catégorielles. Si la variable pour laquelle vous avez calculé le PDP n'est pas corrélée avec les autres caractéristiques, alors les PDP représentent parfaitement comment la variable influence la prédiction en moyenne. Le calcul des graphiques de dépendance partielle a une interprétation causale. Nous intervenons sur une fonctionnalité et mesurons les évolutions des prédictions. Ainsi, nous analysons la relation causale entre la caractéristique et la prédiction. La relation est causale pour le modèle (parce que nous modélisons explicitement le résultat en fonction des caractéristiques) mais pas nécessairement pour le monde réel. 2.3 Inconvénients En raison de techniques de visualisation limitées et de la restriction de la perception humaine à un maximum de trois dimensions, seules une ou deux caractéristiques peuvent raisonnablement être affichées dans un PDP. Le PDP repose sur une hypothèse dindépendance entre les variables, qui est rarement vérifiée en pratique. Si les variables sont corrélées entre elles, la fonction de dépendance partielle produira des points de données irréalistes (par exemple il est peu probable qu'une personne qui mesure 2 mètres pèse 50kg). Nous avons donc, ici, deux problèmes : Le premier problème est que le couple \\((x,y_i)\\) avec \\(x\\) fixé soit invraisemblable : c'est à dire que ce n'est pas quelque chose qui représente la réalité. On peut résoudre ce problème en prenant un intervalle fixé de y qui se tient autour de \\(x\\). Le M-Plot est donc une solution. Le second problème est du à l'influence de \\(x\\) sur \\(y\\). On imagine que \\(x\\) n'a pas d'influence dans la prédiction : on s'attend alors que le PDP soit constant. Mais si \\(y\\) a de l'influence alors on observera un PDP non constant : cela constitue un problème. Afin de le résoudre, les graphiques ALE proposent une bonne solution. Le PDP masque les effets hétérogènes entre les variables, en effet, il ne montre que les effets marginaux moyens. Une solution à ce problème est le graphique ICE présenté plus loin dans ce chapitre. 2.4 En R En R, nous utilisons le package iml pour implémenter les graphique de dépendance partielle. Ci-dessous nous pouvons trouver un exemple de code avec pour boite noire une forêt aléatoire : algo_FA = randomForest(formula, data, importance = TRUE) predictor = Predictor$new(model = algo_FA, data) pdp = FeatureEffect$new(predictor, feature, method = &quot;pdp&quot;, grid.size = 30) p1 = pdp$plot() pdp$set.feature(&quot;nom variable&quot;) p2 = pdp$plot() gridExtra::grid.arrange(p1, p2, nol = 2) Il existe aussi d'autres packages pour implémenter des PDP comme le package pdp ou bien DALEX. Du coté de Python, les PDP sont intégrés dans scikit-learnet. 2.5 Exemples Il existe plusieurs méthodes de traitements pour les modèles de machine learning, on souhaite comparer pour un même jeu de données si les différents algorithmes nous renvoient aux même résultats et surtout essayer de comprendre quelle méthode de machine learning est le plus adaptée pour chaque data frame. 2.5.1 Exemple 1 : Utilisation de deux boites noires différentes comme modèles sur le jeu de données weather Nous choisissons pour notre premier exemple le jeu de données weather du package nycflights13. Il représente les données météorologiques horaires pour les trois aéroports de New York : EWR, LGA et JFK. Nous allons commencer par implémenter une forêt aléatoire puis un algorithme SVM comme modèles de machine learning. Nous voulons prédire la température (en Fahrenheit) des aéroports de New York et utiliser le diagramme de dépendance partielle afin de visualiser les relations que le modèle a apprises. L'influence des caractéristiques que nous avons choisies sur la température sont illustrés dans les figures suivantes. Construction du PDP avec comme boite noire une forêt aléatoire Figure 2.1: PDP pour le modèle de prédiction de la température dans les aéroports de New York avec la méthode des forêts aléatoires. En figure 1, nous obtenons un graphique de dépendance partielle où nous avons prédit la température (en F) dans les différents aéroports de New York avec comme modèle de boite noire une forêt aléatoire. On utilise ce graphique pour visualiser les relations que le modèle a apprises. Nous avons prédit la température à l'aide des variables Point de rosée, Humidité, Vitesse du vent en Miles par heure ainsi que les Aéroports. On rappelle que les marques sur l'axe des \\(x\\) indiquent la distribution des données. On voit donc sur ces graphiques que : Plus le point de rosée augmente, plus la température augmente. Ce qui paraît logique : la rosée apparaît le matin, là où les températures sont les plus faibles. Plus lhumidité augmente, plus la température diminue. On remarque quand même à partir de 75-80% d'humidité un palier, ce qui indique que la température stagne. Plus la vitesse du vent augmente, plus la température diminue. Ce qui paraît logique encore une fois. De plus, on remarque qu'à partir de 30 mph la température stagne. Concernant la variable catégorielle Aéroport, on remarque que peu importe l'aéroport que l'on choisit, ils montrent un effet similaire sur les prévisions du modèle. Visualisation de la dépendance partielle de deux fonctionnalités à la fois Figure 2.2: PDP de la prédiction de la température et l'intéraction de l'humidité et de la vitesse du vent. En figure 2, nous observons un graphique de dépendance partielle pour la prédiction de la température dans les aéroports de New York avec deux fonctionnalités qui sont l'Humidité et la Vitesse du vent avec pour modèle de machine learning une forêt aléatoire. Ce graphique nous montre que plus l'humidité augmente et plus la vitesse du vent augmente, alors la température diminue. Construction du PDP avec comme boite noire un algorithme SVM Figure 2.3: PDP pour le modèle de prédiction de la température dans les aéroports de New York avec la méthode SVM. Nous modélisons à l'aide d'un algorithme SVM notre PDP, de même que pour les conclusions obtenues via une forêt aléatoire, nous pouvons constater les mêmes informations qu'apporte chaque variable et ses effets sur la prédiction de la température : Le point de rosée a un effet positif sur la prédiction de la température L'humidité joue aussi son rôle puisqu'il y a une interaction négative avec la variable température La vitesse du vent influence de manière positive la prédiction de la température L'aéroport n'intervient dans la compréhension de la température puisqu'elle n'a aucun effet sur cette dernière 2.5.2 Exemple 2 : Utilisation de deux boites noires différentes comme modèles sur le jeu de données mtcars Nous choisissons pour notre second exemple la base de données mtcars, qui indique la consommation de carburant (en miles par gallon) de 32 automobiles (modèles 1973-74), à laquelle nous appliquons un modèle de machine learning : les forêts aléatoires. Nous voulons prédire le nombre de cylindres d'une voiture et utiliser le diagramme de dépendance partielle pour visualiser les relations que le modèle a apprises. L'influence des caractéristiques d'une voiture sur le nombre de cylindres prévus sont illustrées dans les figures suivantes. Construction du PDP avec comme boite noire une forêt aléatoire Figure 2.4: PDP pour le modèle de prédiction du nombre de cylindres d'une voiture en fonction de son poids, son déplacement, sa puissance et le rapport du pont arrière avec la méthode des forêts aléatoires. Il est important ici de se placer dans le contexte, nous essayons de prédire le nombre de cylindres présents dans une voiture à l'aide des autres variables présentes dans le dataframe. Comme le nombre de cylindres est fixé, nous avons donc dû créer une variable catégorielle pour classer chaque prédiction. Nous étudions variable par variable pour décrire leurs comportements. La variable Poids, pour chaque niveau, n'influence pas le cylindre d'un véhicule que l'on ait une voiture plus ou moins lourde : cela n'a pas d'importance. Pour ce qui est de la variable Volume du moteur, les probabilités d'affectation à un véhicule à quatre cylindres sont quasiment les mêmes quelque soit le volume du moteur. Il n'y a donc aucune interaction particulière. Pour affecter un véhicule à six cylindres, les probabilités diminuent lorsque le volume du moteur augmente : on note donc un effet négatif. En revanche, les voitures prédites avec huit cylindres voient leurs probabilités d'affectation augmenter via l'augmentation du volume du moteur : nous avons ici une interaction positive. Dans le cadre de la variable Puissance, pour des véhicules prédits à quatre cylindres il n'existe pas de lien pouvant affirmer que l'augmentation de la puissance implique une prédiction des véhicules à quatre cylindres. En revanche, pour prédire un véhicule à six cylindres nous constatons que la puissance joue un rôle dans cette expérience. Lorsqu'elle augmente cela diminue la probabilité de prédire six cylindres dans un véhicule, on en conclut donc un lien négatif. Enfin, pour les grosses cylindrés nous constatons que les probabilités associées à la prédiction de huit cylindres augmentent lorsque la voiture est très puissante, c'est donc un effet positif qui s'exerce entre ces deux variables. Enfin avec la variable mpg, affecter un véhicule à quatre cylindres est impacté par l'augmentation de mpg. En effet, les probabilités de prédire quatre cylindres augmente au fur et à mesure que mpg s'élève aussi. Pour six cylindres nous avons la conclusion inverse, les probabilités de prédire six cylindres diminuent en même temps que mpg. En revanche pour les véhicules affecté à huit cylindres il n'y a aucune importance sur la valeur que peut prendre mpg, les probabilités sont les mêmes. Construction du PDP avec comme boite noire un algorithme SVM Figure 2.5: PDP pour le modèle de prédiction du nombre de cylindres d'une voiture en fonction de son poids, son déplacement, sa puissance et le rapport du pont arrière avec la méthode SVM. De même que pour le traitement précédent nous construisons aussi un PDP à l'aide de lalgorithme SVM. Dans l'hypothèse de savoir si nous obtenons les mêmes informations nous analysons donc nos résultats. Nous reprenons donc notre analyse en nous intéressant au comportement de la variable Poids. Pour un véhicule de plus en plus lourd la probabilité de prédire quatre cylindres diminue elle aussi de plus en plus, nous avons donc le témoignage de l'effet de corrélation négatif. À contrario, pour la prédiction des véhicules de six et huit cylindres, plus leurs poids augmentent et plus leurs probabilités de prédire six ou huit cylindres augmentent. Dans l'idée général nous voyons que les grosses voitures impliquent d'avoir plus de cylindres. Ici, nous regardons l'impact des variables Déplacement et Puissance. Pour ce qui est des voitures prédites à quatre ou six cylindres, elles témoignent le même comportement. En effet, pour des déplacements plus grands les probabilités de prédire que tel véhicule à quatre ou six cylindres décroissent : ce qui met en évidence une influence négative. En revanche, pour les voitures prédites à huit cylindres c'est l'exact opposé, avec de nombreux déplacements cela croît la probabilité d'être affecté à un véhicule comprenant huit cylindres. Ici, on peut voir l'isolement des véhicules citadins standards et les véhicules apprêtés pour faire de gros déplacement. Pour ce qui est de la variable mpg, nous constatons une tendance positive pour prédire des voitures à quatre cylindres, plus on augmente mpg et plus l'on a de chance d'affecter une voiture à quatre cylindres. Ce qui est tout le contraire pour prédire les véhicules à six et huit cylindres qui, eux, ont un effet négatif lorsque mpg augmente. 2.6 Espérance conditionnelle individuelle (ICE) 2.6.1 Définition Un inconvénient du graphique de dépendance partielle est qu'il ne montre que les effets marginaux moyens, en effet il masque les effets hétérogènes entre les variables. Pour résoudre ce problème, il existe les tracés d'espérance conditionnelle individuelle (ICE). Les tracés ICE affichent une ligne pour chaque observation. Ils vont montrer comment la prédiction de l'observation varie lorsqu'une entité varie : c'est donc l'équivalent d'un PDP pour des observations de données individuelles. En conclusion, un PDP est la moyenne des lignes d'un graphique ICE. Une définition plus formelle serait : pour chaque \\(\\{(x_S^{(i)}, x_C^{(i)})\\}_{i=1}^n\\) la courbe \\(\\hat{f}_S^{(i)}\\) est tracée contre \\(x_S^{(i)}\\) tandis que \\(x_C^{(i)}\\) reste fixé. 2.6.1.1 Avantages La méthode ICE, étant une dérivée du PDP, est plus facile à interpréter. En effet, sur le graphe est tracé les prédictions pour chaque instance. Cela permet aussi d'identifier les relations hétérogènes entre \\(x_S^{(i)}\\). 2.6.1.2 Inconvénients Sur les modèles ICE comme nous avons plusieurs tracés sur les graphiques, il n'est par conséquent pas faisable d'ajouter une deuxième variable sur le graphe. Cela rendrait une lecture impossible car nous ne verrons rien suite aux nombreux tracés qui surchargent le graphique. De même que pour les traitements PDP, la méthode ICE nécessite aussi l'hypothèse d'indépendance des variables qui n'est pas vraiment vérifiée dans la pratique. 2.6.1.3 Programmation En R, nous utilisons le package iml pour implémenter les tracés d'espérance conditionnelle individuelle (ICE). Ci-dessous nous pouvons trouver un exemple de code avec pour boite noire une forêt aléatoire : algo_FA = randomForest(formula, data, importance = TRUE) predictor = Predictor$new(model = algo_FA, data) pdp = FeatureEffect$new(predictor, feature, method = &quot;ice&quot;, grid.size = 30) p1 = pdp$plot() pdp$set.feature(&quot;nom variable&quot;) p2 = pdp$plot() gridExtra::grid.arrange(p1, p2, nol = 2) Il existe aussi d'autres packages pour implémenter des ICE comme le package pdp, ICEbox30. Du coté de Python, les ICE sont intégrés dans scikit-learnet. 2.6.2 Exemple : ICE avec le jeu de données weather et comme boite noire une forêt aléatoire Figure 2.6: ICE pour le modèle de prédiction de la température (en F) avec la méthode des forêts aléatoires. En figure 6, nous obtenons un tracé ICE où nous avons prédit la température (en F) dans les différents aéroports de New York avec comme modèle de boite noire une forêt aléatoire. Chaque ligne représente une température. Toutes les instances représentées dans le modèle ICE nous indiquent les mêmes informations que dans le modèle PDP : Le point de rosée a toujours un effet positif sur la prédiction de la température L'humidité joue aussi son rôle puisqu'il y a une tendance négative avec la variable température La vitesse du vent influence de manière positive la prédiction de la température "],["importance-des-variables-dans-les-modèles-prédictifs.html", "Chapitre 3 Importance des variables dans les modèles prédictifs 3.1 Définition 3.2 Avantages 3.3 Inconvénients 3.4 En R 3.5 Exemples", " Chapitre 3 Importance des variables dans les modèles prédictifs 3.1 Définition La fonction de permutation mesure l'augmentation de l'erreur de prédiction du modèle après la permutation des valeurs de la fonction, ce qui rompt la relation entre la fonction et le résultat réel. Son énorme avantage est quelle sapplique à nimporte quel type de modèle prédictif. Une caractéristique est importante si le mélange de ses valeurs augmente l'erreur du modèle, alors dans ce cas, le modèle s'est appuyé sur cette caractéristique pour la prédiction. Une fonction est sans importance si le mélange de ses valeurs laisse l'erreur de modèle inchangée, en effet dans ce cas, le modèle a ignoré la fonction pour la prédiction. En résumé, l'algorithme permet de modifier les valeurs des variables pour voir si l'erreur augmente ou diminue. On introduit, ici, l'algorithme d'importance des caractéristiques de permutation basé sur Fisher, Rudin et Dominici (2018). Pour neutraliser la variable, ils préconisent de mélanger aléatoirement les valeurs à lintérieur du vecteur et, par conséquent, de casser le lien quelle peut entretenir avec la classe à prédire (et les autres variables par la même occasion). On retranscrit l'algorithme : Entrée: modèle formé \\(f\\), matrice de caractéristiques \\(X\\), vecteur cible \\(y\\), mesure d'erreur \\(L(y, f)\\). Estimer l'erreur d'origine du modèle \\(e^{orig} = L(y, f(X))\\) (par exemple, erreur quadratique moyenne) Pour chaque caractéristique \\(j = 1,...,p\\) faire: Générez une matrice de caractéristiques \\(X^{perm}\\) en permutant la caractéristique \\(j\\) dans les données \\(X\\). Cela rompt l'association entre la caractéristique \\(j\\) et le résultat réel \\(y\\). Estimer l'erreur \\(e^{perm} = L(Y, f(X^{perm}))\\) sur la base des prédictions des données permutées. Calculer l'importance de la fonction de permutation \\(FI^j = e^{perm} / e^{orig}\\). Alternativement, la différence peut être utilisée: \\(FI^j = e^{perm} - e^{orig}\\). Trier les entités par \\(FI\\) décroissante. Limportance des variables peut être estimée en modélisation (sur léchantillon dapprentissage) ou en prédiction (sur léchantillon test). Dans les deux cas, les principales étapes sont les mêmes : calculer le taux derreur de référence, calculer ensuite le même indicateur en neutralisant tour à tour chaque variable prédictive et enfin former le ratio entre les deux valeurs. L'idée de la permutation étant de mesurer l'erreur quadratique moyenne/absolue des prédictions qui ont été faites après permutation de l'ensemble des \\(X_1^{(i)}\\) où \\(i = 1,..,n\\). La fonction calcule plusieurs erreurs de prédiction pour une même variable dont les valeurs ont été affectées à d'autres individus \\(i\\). Le but est de comparer si le mélange des valeurs provoque un effet sur l'erreur du modèle, donc savoir si l'on peut traduire le comportement d'une variable \\(Y\\) affecté à une variable \\(X\\) : Le mélange des valeurs de \\(X_1^{(i)}\\), provoquant une augmentation de l'erreur du modèle, traduit l'importance de \\(X_1\\) car cela indique que le modèle s'est appuyé sur cette variable pour prédire \\(Y\\). Le mélange des valeurs de \\(X_1^{(i)}\\), n'ayant aucun effet sur laugmentation de l'erreur du modèle, montre qu'il n'y a eu aucune interaction pour la prédiction et donc n'intervient pas dans le traitement. 3.2 Avantages L'un des avantages pour la Permutation Features Importance est qu'il casse les effets d'interactions issues des variables explicatives \\(X_i\\) très fortement corrélées entre elles. En effet, puisqu'elle permute les données, de ce fait cette procédure compresse l'information et augmente implicitement l'erreur dans notre modèle. C'est aussi un avantage sur le temps de traitement des données, il n'est pas nécessaire d'obtenir/ modifier/ supprimer/ entraîner le modèle étudié et puis, ici l'expérience est de garder un modèle fixe et de faire varier pour chaque \\(X_i\\) leurs positions. L'utilisation de la Permutation Features Importance reste assez simpliste, ne nécessitant pas d'ensemble d'apprentissage car les variables qui seront dites impactantes pour une analyse PFI ne le serons pas sur une nouvelle expérimentation après suppression de ces dernières. 3.3 Inconvénients L'utilisation de la permutation se révèle pratique pour interpréter un ensemble de données. Néanmoins elle regorge de points négatifs lorsque nous sommes dans une situation où des variables explicatives \\(X_i\\) s'avèrent être corrélées. Prenons un exemple pour mieux comprendre les phénomènes de corrélations à l'aide du jeu de données mtcars : Nous avons pris les variables disp ainsi que wt qui sont fortement corrélées puisque \\(cor(X_{disp} , X_{wt}) = 0.888\\) pour expliquer le gros problème de la permutation. Nous avons mis en évidence à l'aide du graphique de gauche un lien linéaire qui existe, pour comprendre les limites de cette méthode dinterprétation nous avons permuté les données issues de disp. Enfin, nous projetons le graphique entre la variable \\(X_{disp}\\) permutée et \\(X_{wt}\\) pour mettre en évidence le gros défaut de la permutation. En effet, on constate que la linéarité a disparu dans le nuage de points de droite. Or, pour calculer l'importance de la variable \\(X_{disp}\\) le mélange provoqué par la permutation va aboutir à des calculs de prédictions complètement fragiles dans des zones supposées aberrantes puisque si l'on s'intéresse aux individus \\(i\\) de la partie haute-gauche \\(i = {23,11;31,14,15,17}\\) ceux sont des points aberrants qui vont entrer dans les calculs pour prédire \\(Y\\) et saboter l'interprétation finale. 3.4 En R En R, nous utilisons toujours le package iml pour implémenter les permutations. Ci-dessous nous pouvons trouver un exemple de code avec pour boite noire un arbre décisionnel : tree &lt;- rpart(Petal.Length ~ .,data = iris) Y &lt;- iris$Petal.Length X &lt;- iris[-which(names(iris) == &quot;Petal.Length&quot;)] mod &lt;- Predictor$new(tree, data = X, y = Y) imp &lt;- FeatureImp$new(mod, loss = &quot;mse&quot;) plot(imp) Il existe aussi d'autres packages pour implémenter des PFI comme le package vip ou bien DALEX. Du coté de Python, les PFI sont intégrés dans alibi. 3.5 Exemples 3.5.1 Exemple 1 : PFI sur les données iris Nous utilisons le jeu de données iris contenu dans R afin d'effectuer une Permutation Features Importance. Nous allons implémenter un arbre décisionnel que nous comparerons à une forêt aléatoire après avoir regardé l'importance des variables en fonction du modèle de boîte noire utilisé. 3.5.1.1 Construction de notre arbre décisionnel Figure 3.1: Graphique d'une Permutation Features Importance pour un arbre décisionnel appliqué au jeu de données iris. Ici, nous avons un graphique de notre Permutation Features Impact avec comme mesure d'erreur mse (il en existe d'autres). Il représente l'erreur quadratique moyenne définie par \\(MSE = \\left( \\frac{1}{n} \\right) \\times\\sum_{i=1}^{n} (Y_i-\\hat{Y_i})^2\\). Les observations à faire pour ce genre de graphique est d'observer les variables dites importantes ou celles qui ont un comportement impactant \\(Y\\) à travers le comportement de notre fonction de permutation \\(FI_j = e^{perm} - e^{orig}\\) où les \\(e^{perm}\\) et \\(e^{orig}\\) représentent leurs erreurs quadratiques respectives. Lorsque les variables comme Sepal.Width sont relativement proches de 0, cela indique qu'elles n'ont pas vraiment dimpact sur la prédiction de \\(Y\\). Autrement dit, l'interaction est très faible voire inexistante. Le constat subsistant est que la fonction de permutation \\(FI\\) est nulle, autrement dit le modèle d'apprentissage n'apporte pas plus d'informations qu'avec ou sans permutation, via le fait que l'erreur du modèle n'augmente pas \\(e^{perm} = e^{orig}\\). De ce fait la variable Sepal.Width n'est donc pas importante. En revanche pour les variables comme Species nous voyons d'office qu'elle est importante sur la prédiction de Petal.Length. En effet \\(e^{perm} &gt; e^{orig}\\) est très concret, doù l'augmentation d'erreur plus grande après permutation impliquant une explication partielle mais réelle pour prédire \\(Y :\\) La longueur des pétales. Pour ce qui est des variables Sepal.Length et Petal.Width, nous notons des fonctions de permutations \\(FI &gt; 0\\) mais néanmoins très faibles elles demeurent être très proches de 0, cela met en évidence la question de seuil à prendre en compte pour vraiment qualifier une variable dite importante. Néanmoins une maigre part d'information s'ajoute à notre expérience afin de prédire Petal.Length. Comparaison des modèles PDP/ICE vs Permutation Feature Importance Ce qui est intéressant de voir ici est que contrairement à l'utilisation du PDP ou son dérivé l'ICE (chapitre 2), on n'obtient pas nécessairement les mêmes résultats : le graphique de l'importance des variables met en lumière une seule et unique variable définie comme importante (Species). Chaque méthode a ses contraintes et ses avantages cela diffère donc de ce que l'on souhaite interpréter/conclure en fonction de nos types de données. Nous allons utiliser une autre méthode de classification pour décrire les interprétations faites par la méthode de permutation des variables. 3.5.1.2 Construction de notre forêt aléatoire Figure 2.1: Graphique d'une Permutation Features Importance pour une forêt aléatoire appliquée au jeu de données iris. Voici les résultats issus de la permutation sur des données simulées à l'aide d'une forêt aléatoire. Contrairement à ce que nous apportait comme informations larbre décisionnel, ici, nous constatons distinctement que les variables Species, Sepal.Length et Petal.Width génèrent une erreur plus grande lorsque le modèle est permuté témoignant de l'explication qu'elles apportent chacune d'entre elles sur Petal.Length, on a donc \\(e^{perm} &gt; e^{orig}\\) avec une différence assez notable. Pour la variable Sepal.Width, on voit ici que la fonction de permutation \\(FI^j = e^{perm} - e^{orig} = 0\\). On remarque qu'aucune explication n'est apportée en plus y compris sous l'effet de la permutation, elle ne suscite donc pas grand intérêt dans l'interaction entre \\(X^2_i\\) et \\(Y_i\\). 3.5.1.2.1 Comparaison Permutation Feature Importance entre arbre décisionnel et forêt aléatoire Pour un même type de traitement sur les boîtes noires avec deux modèles de classifications différents, nous avons des résultats asymétriques pour un modèle d'erreur identique, le MSE. En effet, avec l'utilisation de larbre décisionnel, l'unique variable Species a été qualifiée d'importante pour la prédiction de Petal.Length. Tandis que les résultats issues des forêts aléatoires mettent en évidence trois variables importantes Species, Sepal.Length et Petal.Width, ce qui est complètement différent et relève de la complexité des choix de constructions pour nos classifications mais aussi pour nos traitements de boîtes noires. Toutes ces méthodes amènent à penser quels critères sont indicatifs pour anticiper, quels traitements sont à proscrire ou à prescrire suivant notre ensemble d'apprentissage. 3.5.2 Exemple 2 : PFI sur les données fifa Pour une étude comparative avec une base de donnée plus conséquente, nous prenons donc la base de données fifa regroupant les notes des joueurs professionnels ainsi que leurs caractéristiques. Comme dans le cas précédent, nous effectuons une forêt aléatoire ainsi qu'un arbre décisionnel pour comparer les sorties. Figure 2.2: Graphique de deux Permutation Features Importance pour une forêt aléatoire ainsi que pour un arbre décisionnel appliqués au jeu de données fifa. Au vu des ces deux graphiques, ce que nous voyons de prime abord est l'importance des variables que renvoie notre PFI sur notre forêt aléatoire. Il y a juste besoin de regarder les échelles pour se rendre compte à quel point l'erreur engendré par la permutation via une forêt aléatoire est plus grande que celle issue de larbre décisionnel. Par conséquent, la mesure de l'importance est beaucoup plus démonstrative pour les forêts aléatoires. Ce qu'il faut ajouter en plus, c'est que la méthode des forêts aléatoires détecte beaucoup plus de variables dites importantes à contrario de l'arbre de décision qui ne dégage que trois variables importantes. Les critères de mesure d'erreur étant les mêmes basées sur le mean square error, l'arbre génère beaucoup moins d'informations que ce que les forêts aléatoires peuvent en apporter. 3.5.2.1 Ouverture sur le choix des variables Cette méthode d'interprétabilité relevant des avantages et des inconvénients, ici nous avons vu l'implémentation de la Permutation Features Importance. Cela nous a permis d'affirmer ou infirmer certains propos pour certaines variables, en revanche il pose un problème de validation des données pour des fonction de permutation \\(FI^j \\sim 0\\). En effet, là se pose des tests d'hypothèses statistiques à effectuer en aval pour valider mathématiquement nos résultats lorsqu'il demeure une augmentation d'erreur de modèle après permutations mais de valeurs qui émettent le doute. "],["substitut-local-lime.html", "Chapitre 4 Substitut local (LIME) 4.1 Définition 4.2 Avantages 4.3 Inconvénients 4.4 En R 4.5 Exemples", " Chapitre 4 Substitut local (LIME) 4.1 Définition Les modèles de substitutions locaux représentent une nouvelle méthode d'interprétabilité qui explique chaque prédiction individuelle lors des traitements de machine learning. Cet outil est une spécificité du modèle de substitution global. Ici, en utilisant la méthode LIME on cherche à expliquer bout à bout des prévisions pour tout type d'individu. La méthode LIME cherche à comprendre pour un ensemble de données pourquoi le modèle de machine learning a fait une certaine prédiction. Ainsi, elle teste pour chaque prédiction ce qu'il se passe lorsque les données sont perturbées/variées. De ce fait, elle génère un nouvel ensemble de données perturbées ainsi que les prédictions qu'il en découle pour définir un ensemble dapprentissage, avec une pondération des instances perturbées sur l'instance d'intérêt en fonction de la proximité avec cette dernière. On peut noter que cette méthode s'applique sur n'importe quelle méthode de machine learning. Elle présente donc l'explication d'une prédiction locale menant droit à de bonnes approximations locales. En revanche, il peut s'avérer que les approximations ne soient pas bonnes pour un modèle global, cela est défini par &quot;Local fidelity&quot;. Les modèles de substitutions sont exprimés ainsi : \\[ explanation(x) = \\arg\\max_{g \\in G} \\ L(f,g,\\pi_x) + \\Omega(g) \\] 4.2 Avantages Ici, nous nous intéressons uniquement à LIME pour des données tabulaires. Sachez, tout de même, quil existe des algorithmes LIME pour des données textuelles ainsi que pour des images. Cette méthode est une des rares pouvant être élargie à différents types de données. Le &quot;Local Fidelity&quot; est une mesure de précision sur la fiabilité du modèle interprétable. Il sert à expliquer les prédictions locales focalisées sur notre instance d'intérêt. La méthode LIME est très rapide en coût de calcul, et elle est implémentée sur les logiciels R et Python dans différents packages. On notera que lors de l'utilisation du package iml, R utilise directement les régressions linéaires lorsque nous appelons la fonction ce qui réduit les temps de calculs et de codes. 4.3 Inconvénients La méthode LIME se base sur le choix de noyaux (estimations par densité). Cela implique que son utilisation doit être très contrôlée. En effet, elle nécessite pour chaque application de tester avec plusieurs paramètres de noyaux qui diffèrent afin de voir si les explications apportées s'avèrent être interprétables. Le &quot;Local Fidelity&quot; peut poser problème dans le cadre d'une analyse globale. Effectivement, nous avons dit précédemment que c'était une mesure fiable pour expliquer les prédictions locales. En revanche, dans sa globalité elle n'est pas forcément parlante et interprétable : ce qui met en évidence le contraste local vs global. De même que pour les fonctions de permutations, nous avons aussi un problème de corrélations entre variables endogènes puisque de nouvelles instances sont échantillonnées au cours de la procédure. Cela a pour effet de ne pas tenir compte des interactions entres variables corrélées qui constitueront des prédictions sur des ensembles de points de nouveaux incohérents. Il existe aussi le problème d'instabilité des explications. Les données permutées qui forment l'échantillon d'apprentissage rendent par défaut une explication instable, puisqu'elle ne garantit en rien qu'une fois le processus répété nous obtenons les mêmes résultats. La méthode LIME se trouve être prometteuse dans sa capacité à essayer d'expliquer ce qu'il se déroule dans les boîtes noires, ainsi que pour la compréhension de nos prédictions. Néanmoins, elle reste encore fragile et nécessite d'être bien construite pour renforcer cette méthode prometteuse et de l'appliquer sans soucis. 4.4 En R En R, nous utilisons toujours le package iml pour implémenter la méthode LIME. Ci-dessous nous pouvons trouver un exemple de code avec pour boite noire une forêt aléatoire : algo_FA = randomForest(formula, data, importance = TRUE) predictor = Predictor$new(model = algo_FA, data) lime.explain = LocalModel$new(predictor, x.interest = , k = 8) #k règle le nb de variable lime.explain$results %&gt;% ggplot(aes(x = reorder(feature.value, effect), y = effect, fill = feature.value)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(x=&quot;&quot;, y=&quot;&quot;, fill=&quot;&quot;) + theme(legend.position=&#39;none&#39;) #ou tout simplement : plot(lime.explain) Il existe aussi d'autres packages pour implémenter une méthode LIME comme le package lime. Du coté de Python, les méthodes LIME sont intégrés dans lime. 4.5 Exemples 4.5.1 Exemple 1 : LIME sur le jeu de données weather On utilise le jeu de données weather du package nycflights13. Il représente les données météorologiques horaires pour les trois aéroports de New York : EWR, LGA et JFK. Construction de LIME avec comme boite noire une forêt aléatoire Figure 3.1: Graphique LIME pour la prédiction de la température des aéroports de New York avec une forêt aléatoire. Après avoir implémenté notre méthode LIME sur la prédiction de la température des aéroports, il en ressort ce graphique témoignant pour chaque variable explicative les effets de ces dernières, à l'aide des régressions effectuées en amont. La barre des abscisses témoigne de l'effet que peut avoir une variable sur \\(Y\\). Pour ce qui est de la variable dewp nous avons un effet/coefficient positif impactant la montée de température lorsque cette dernière augmente aussi. Pour le reste, seul les variables pressure, humid et visib constituent un impact négatif sur la température lorsqu'elles augmentent. Cela a donc pour effet de diminuer la température dans les aéroports de New-York. Quant aux autres variables, LIME a décrété qu'elles n'avaient pas/peu de poids sur la température. Construction de LIME avec comme boite noire l'agorithme des KNN Figure 2.1: Graphique LIME pour la prédiction de la température des aéroports de New York avec l'algorithme des KNN. Nous pensons qu'il est intéressant de comparer deux modèles de boites noires sur le même jeu de données en appliquant la méthode LIME. Nous avons donc procédé à l'algorithme des KNN et à notre grande surprise nous obtenons les mêmes interprétations faites auparavant. On peut cependant remarquer que les graphiques ne sont pas totalement égaux. En effet, lorsque l'on utilise la méthode LIME, il faut choisir sur quelle ligne de notre jeu de données nous nous basons (x.interest). Ici, nous avons choisi la première ligne pour l'algorithme des forêts aléatoires. Tandis que pour l'algorithme des KNN, nous avons choisi la troisième ligne : ce qui explique les différences. Il est important de retenir que nous obtenons les mêmes interprétations peu importe la ligne choisie et l'algorithme. Exemple 2 : LIME sur le jeu de données iris Construction de LIME avec comme boite noire une forêt aléatoire Figure 2.2: Graphique LIME pour la prédiction de la longueur des sépales d'une fleur avec une forêt aléatoire. On sintéresse maintenant au jeu de données iris. Nous avons trois variables explicatives. Après avoir implémenté notre méthode LIME sur la prédiction de la longueur des sépales, on remarque que seulement Sepal.Width et Petal.Length possèdent un impact positif sur cette prédiction. En revanche, Petal.Width n'est pas impliquée sur la prédiction de Sepal.Lenght. Construction de LIME avec comme boite noire l'agorithme des KNN Figure 2.3: Graphique LIME pour la prédiction de la longueur des sépales d'une fleur avec l'algorithme des KNN. Ici, nous observons comme précédemment les mêmes interprétations. Les graphiques sont totalement égaux, puisque nous avons choisi de prendre en compte les mêmes lignes d'observations. Grâce à ces deux exemples, on peut dire que peu importe l'algorithme utilisé nous obtiendrons les mêmes résultats et interprétation après l'application de la méthode LIME. "],["références.html", "Chapitre 5 Références", " Chapitre 5 Références Références pour les jeux de données utilisés mtcars est un jeu de données inclus dans R. Les données ont été extraites du magazine américain Motor Trend de 1974 et comprennent la consommation de carburant et 10 aspects de la conception et des performances des automobiles pour 32 automobiles (modèles de 1973 à 1974). weather est un jeu de données inclus dans le package nycflights13 dans R. Ce sont les données météorologiques horaires pour les aéroports de New York : LGA, JFK et EWR. iris est un jeu de données inclus dans R. Cet ensemble de données sur l'iris (de Fisher ou d'Anderson) donne les mesures en centimètres des variables longueur et largeur des sépales et longueur et largeur des pétales, respectivement, pour 50 fleurs de chacune des 3 espèces d'iris. Les espèces sont Iris setosa, versicolor et virginica. fifa est un jeu de données non inclus dans R. Il est disponible à l'adresse suivante : https://www.kaggle.com/thec03u5/fifa-18-demo-player-dataset. Elle regroupe les notes de joueurs professionnels jouant au football ainsi que leurs caractéristiques. Référence au package iml https://cran.r-project.org/web/packages/iml/iml.pdf Lien du livre Christoph Molnar. 2020. Interpretable Machine Learning : A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/ Liens utiles https://perso.math.univ-toulouse.fr/mllaw/home/statisticien/explicabilite-des-decisions-algorithmiques/methodes-dapprentissage-machine/ Xie, Yihui. 2020. Bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown "]]
